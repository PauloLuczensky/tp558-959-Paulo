# ğŸ“˜ Projeto TP558 â€” Federated Learning + XGBoost + Optuna + XAI (Multiclasse)

Este projeto implementa um pipeline completo de **classificaÃ§Ã£o multiclasse** utilizando:

- **Aprendizado Federado (Federated Learning â€“ FL)**
- **XGBoost** como modelo base
- **Optuna** para otimizaÃ§Ã£o de hiperparÃ¢metros
- **TÃ©cnicas de Explainable AI (XAI)**
- AvaliaÃ§Ã£o, comparaÃ§Ã£o e interpretaÃ§Ã£o dos resultados

O notebook demonstra desde o carregamento do dataset atÃ© a explicaÃ§Ã£o dos modelos, sendo ideal para aplicaÃ§Ãµes que exigem privacidade, desempenho otimizado e interpretabilidade.

---

## ğŸ“‚ Estrutura Geral do Projeto

### 1. ğŸ“Š Dataset
Nesta etapa ocorre:
- Carregamento dos dados
- Limpeza e prÃ©-processamento
- SeleÃ§Ã£o e engenharia de features (quando aplicÃ¡vel)
- DivisÃ£o em treino, validaÃ§Ã£o e teste
- NormalizaÃ§Ã£o/PadronizaÃ§Ã£o dos atributos

---

### 2. ğŸ§  Treinamento (FL + XGBoost + Optuna)

#### ğŸ”¹ Federated Learning
ImplementaÃ§Ã£o de aprendizado federado, permitindo treinar modelos em diferentes â€œclientesâ€ sem compartilhar dados sensÃ­veis.

Fluxo tÃ­pico:
1. SeparaÃ§Ã£o dos dados em mÃºltiplos clientes
2. Treinamento local com XGBoost
3. AgregaÃ§Ã£o dos modelos (ex.: FedAvg)
4. RepetiÃ§Ã£o por vÃ¡rias rodadas federadas

#### ğŸ”¹ XGBoost
O modelo de boosting utilizado para classificaÃ§Ã£o multiclasse durante as rodadas locais de aprendizado.

#### ğŸ”¹ Optuna â€” Hyperparameter Tuning
Utilizado para encontrar os melhores hiperparÃ¢metros, como:
- `eta`
- `max_depth`
- `min_child_weight`
- `gamma`
- `subsample`
- `colsample_bytree`
- Entre outros parÃ¢metros do XGBoost

Optuna otimiza automaticamente para maximizar a mÃ©trica escolhida (acurÃ¡cia, F1-score, etc.).

---

### 3. ğŸ“ˆ Resultados
SÃ£o apresentados:
- AcurÃ¡cia e mÃ©tricas por classe
- Matriz de confusÃ£o
- Curvas de desempenho (quando aplicÃ¡vel)
- ComparaÃ§Ã£o entre modelo federado e modelo centralizado
- HiperparÃ¢metros Ã³timos encontrados pelo Optuna

---

### 4. ğŸ” Aplicando XAI (Explainable AI)
ExplicaÃ§Ãµes do comportamento do modelo utilizando tÃ©cnicas como:
- **SHAP values**
- **Feature importance**
- **Summary plots**
- **Decision plots**

Estas explicaÃ§Ãµes permitem interpretar:
- Quais atributos mais influenciam o modelo
- Como as decisÃµes sÃ£o tomadas para cada classe
- A lÃ³gica interna do XGBoost apÃ³s o treinamento federado

---

## ğŸš€ Tecnologias Utilizadas

- Python
- XGBoost
- Optuna
- SHAP
- Pandas / NumPy
- Matplotlib / Seaborn
- Framework ou implementaÃ§Ã£o prÃ³pria de **Federated Learning**

---

## â–¶ï¸ Como Executar

1. Instale as dependÃªncias:

```bash
pip install xgboost optuna shap pandas numpy matplotlib
```

## ğŸ¯ Objetivo do Projeto

O objetivo Ã© demonstrar como integrar:

- Aprendizado Federado

- Modelagem com XGBoost

- OtimizaÃ§Ã£o de hiperparÃ¢metros com Optuna

- Explicabilidade usando XAI em um fluxo robusto de classificaÃ§Ã£o multiclasse capaz de:

    - Preservar privacidade dos dados

    - Maximizar desempenho

    - Aumentar transparÃªncia do modelo

## ğŸ“ Autores


Autores :

Alessandra Carolina Domicianoâ€‹

Paulo Otavio Luczensky de Souzaâ€‹
